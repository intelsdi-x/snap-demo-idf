## Working with Kubernetes

In this section we will:

* start Kubernetes VMs
* deploy a container as a service
* explore kubernetes resources:
    * pod: group of one or more containers
    * service: policy on how to access logical set of pods
    * replica sets: number of pod 'replicas'
    * deployment: declarative definition of pod version and replica sets
* cleanup kubernetes resources

### Start Kubernetes

If the output does not match the examples below, see troubleshooting section for resolution.

NOTE: commands executed in the vagrant VMs are prefaced with the VM name `k8s-01 $` instead of just `$`

```
$ git clone https://github.com/nanliu/kargo.git
$ cd kargo
$ git checkout idf
```

Bring them up the VMs:
```
$ vagrant up
Bringing machine 'k8s-01' up with 'virtualbox' provider...
Bringing machine 'k8s-02' up with 'virtualbox' provider...
Bringing machine 'k8s-03' up with 'virtualbox' provider...
...
```

Once the VMs are provisioned ssh into k8s-01 master:
```
$ vagrant ssh k8s-01 -- -L 8080:localhost:8080 -L 8181:localhost:8181 -L 5000:localhost:5000
```
NOTE: The ssh port forwarding will allow kubectl command to work directly the local system, and docker image push.

checker cluster-info:
```
k8s-01 $ kubectl cluster-info
Kubernetes master is running at http://localhost:8080
dnsmasq is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/dnsmasq
kubedns is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kubedns
```

verify component status:
```
k8s-01 $ kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health": "true"}
etcd-2               Healthy   {"health": "true"}
etcd-1               Healthy   {"health": "true"}
```

verify etcd is running and flannel network available (should have three subnets, please wait three min after system boot):
```
k8s-01 $ etcdctl ls /cluster.local/network/subnets
/cluster.local/network/subnets/10.233.71.0-24
/cluster.local/network/subnets/10.233.109.0-24
/cluster.local/network/subnets/10.233.85.0-24
```

CAUTION!!!: Only run the following commands if cluster-info does not show kubedns running or no flannel network is listed:
```
$ vagrant reload && vagrant provision
```

review and ensure all pods are running:
```
k8s-01 $ kubectl get pods --all-namespaces

NAME                             READY     STATUS    RESTARTS   AGE
dnsmasq-k1qkq                    1/1       Running   0          1h
dnsmasq-kez7o                    1/1       Running   0          1h
dnsmasq-x48kg                    1/1       Running   0          1h
flannel-k8s-01                   2/2       Running   4          1h
flannel-k8s-02                   2/2       Running   2          1h
flannel-k8s-03                   2/2       Running   3          1h
kube-controller-manager-k8s-01   1/1       Running   0          1h
kube-controller-manager-k8s-02   1/1       Running   1          1h
kube-proxy-k8s-01                1/1       Running   2          1h
kube-proxy-k8s-02                1/1       Running   2          1h
kube-proxy-k8s-03                1/1       Running   2          1h
kube-scheduler-k8s-01            1/1       Running   0          1h
kube-scheduler-k8s-02            1/1       Running   0          1h
kubedns-cvsbm                    4/4       Running   0          1h
```

push images to containers:
```
$ sudo push_images
The push refers to a repository [localhost:5000/grafana-snap]
a3ed72e93f70: Pushed
ac76e12a4693: Pushed
...
```

### Running containers

start a nginx container:
```
$ kubectl run hello-minikube --image=localhost:5000/echoserver:1.4 --port=8080
deployment "hello-minikube" created
```

verify pod is running:
```
$ kubectl get pods
NAME                              READY     STATUS    RESTARTS   AGE
hello-minikube-1344652226-nbn3a   1/1       Running   0          58s

$ kubectl describe pod hello-minikube-1175569297-mjb1u
Name:        hello-minikube-1175569297-mjb1u
Namespace:    default
Node:        k8s-03/172.17.8.103
Start Time:    Wed, 27 Jul 2016 14:34:21 -0700
Labels:        pod-template-hash=1175569297
        run=hello-minikube
Status:        Running
IP:        10.233.114.5
Controllers:    ReplicaSet/hello-minikube-1175569297
Containers:
  hello-minikube:
    Container ID:             docker://a8a27755799bb73d9e428ea8b18d2217af2e5a54040177674ae7b6c0d333a977
    Image:                    localhost:5000/echoserver:1.4
    Image ID:                 docker://sha256:a90209bb39e3d7b5fc9daf60c17044ea969aaca0333d672d8c7a34c7446e7ff7
    Port:                     8080/TCP
    State:                    Running
      Started:                Wed, 27 Jul 2016 14:34:22 -0700
    Ready:                    True
    Restart Count:            0
    Environment Variables:    <none>
Conditions:
  Type        Status
  Initialized     True
  Ready     True
  PodScheduled     True
Volumes:
  default-token-8umfn:
    Type:    Secret (a volume populated by a Secret)
    SecretName:    default-token-8umfn
QoS Tier:    BestEffort
Events:
  FirstSeen    LastSeen    Count    From            SubobjectPath            Type        Reason    Message
  ---------    --------    -----    ----            -------------            --------    ------    -------
  16m        16m        1    {default-scheduler }                    Normal        Scheduled    Successfully assigned hello-minikube-1175569297-mjb1u to k8s-03
  16m        16m        1    {kubelet k8s-03}    spec.containers{hello-minikube}    Normal        Pulled    Container image "localhost:5000/echoserver:1.4" already present on machine
  16m        16m        1    {kubelet k8s-03}    spec.containers{hello-minikube}    Normal        CreatedCreated container with docker id a8a27755799b
  16m        16m        1    {kubelet k8s-03}    spec.containers{hello-minikube}    Normal        StartedStarted container with docker id a8a27755799b
```

### Exercise

* What other hello-minikube resource exists?
* What other pods are running on the node hello-minikube running on?
* What's the resource limits of each container?

### Accessing container services:

expose deployment service:
```
$ kubectl expose deployment hello-minikube --type=NodePort
service "hello-minikube" exposed
```

access nginx service:
```
$ kubectl describe service hello-minikube
Name:             hello-minikube
Namespace:        default
Labels:           run=hello-minikube
Selector:         run=hello-minikube
Type:             NodePort
IP:               10.233.23.176
Port:             <unset> 8080/TCP
NodePort:         <unset> 32274/TCP
Endpoints:        10.233.114.5:8080
Session Affinity: None
No events.

k8s-01 $ curl 10.233.23.176:8080
CLIENT VALUES:
client_address=172.17.8.101
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://10.233.23.176:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=10.233.23.176:8080
user-agent=curl/7.47.0
BODY:
-no body in request
```

### Exercise

* What happens when the pod is deleted `kubectl delete pod ${hello-minikube_podname}`?
* What happens when the deployment is deleted?
    * What's the pod status?
    * What's the service status?
